
from qdrant_client import QdrantClient
from embedding import generate_query_embedding, generate_embeddings
from pdf_processing import extract_and_process_pdf
from retrieval import rerank_with_cross_encoder
from vectordb import upload_to_qdrant, delete_collection
from rewrite import enhance_query
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
import streamlit as st
import os
import numpy as np
import re
import asyncio
import json
import pandas as pd
from io import StringIO

from qdrant_client import QdrantClient
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
try:
    asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

qdrant_host = "127.0.0.1"
qdrant_port = 6333
temperature = 0.1

@st.cache_resource
def init_llm(api_key):
    return ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        google_api_key=api_key,
        temperature=temperature
    )

@st.cache_resource
def init_qdrant():
    return QdrantClient(host=qdrant_host, port=qdrant_port)

prompt_template = ChatPromptTemplate.from_template(
"""
B·∫°n l√† m·ªôt RAG AI AGENT th√¥ng minh t√™n l√† QUIZ_CHAT .
N·∫øu c√≥ th√¥ng tin ng·ªØ c·∫£nh b√™n d∆∞·ªõi, h√£y s·ª≠ d·ª•ng **ch√≠nh x√°c** n·ªôi dung trong ƒë√≥ ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
N·∫øu ng·ªØ c·∫£nh ch·ª©a b·∫£ng, h√£y ph√¢n t√≠ch b·∫£ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√°c h√†ng v√† c·ªôt t∆∞∆°ng ·ª©ng.
T√™n c·ªôt v√† h√†ng ƒë√£ ƒë∆∞·ª£c cung c·∫•p trong b·∫£ng. H√£y s·ª≠ d·ª•ng ch√∫ng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c.

L·ªãch s·ª≠ h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥:
{history}

Ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu (n·∫øu c√≥):
{context}

C√¢u h·ªèi hi·ªán t·∫°i:
{question}

Y√™u c·∫ßu:
- N·∫øu ng·ªØ c·∫£nh l√† b·∫£ng, h√£y tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu trong b·∫£ng. Tr√≠ch xu·∫•t th√¥ng tin t·ª´ c√°c h√†ng v√† c·ªôt li√™n quan.
- Tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m, ng·∫Øn g·ªçn v√† b·∫±ng ti·∫øng Vi·ªát.
- Tuy·ªát ƒë·ªëi kh√¥ng b·ªãa th√¥ng tin n·∫øu d√πng ng·ªØ c·∫£nh.
- N·∫øu kh√¥ng c√≥ ng·ªØ c·∫£nh, h√£y tr·∫£ l·ªùi t·ª± nhi√™n nh∆∞ m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán.

C√¢u tr·∫£ l·ªùi:
"""
)


prompt_direct_template = ChatPromptTemplate.from_template(
"""D·ª±a v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i d∆∞·ªõi ƒë√¢y v√† ki·∫øn th·ª©c c·ªßa b·∫°n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi.
N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y th√†nh th·∫≠t n√≥i 'T√¥i kh√¥ng bi·∫øt'.

L·ªãch s·ª≠ h·ªôi tho·∫°i:
{history}

C√¢u h·ªèi:
{question}

Y√™u c·∫ßu:
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c b·∫±ng ti·∫øng Vi·ªát
- Ch·ªâ tr·∫£ l·ªùi 'T√¥i kh√¥ng bi·∫øt' khi th·ª±c s·ª± kh√¥ng c√≥ th√¥ng tin
- Kh√¥ng t·ª± b·ªãa c√¢u tr·∫£ l·ªùi

Tr·∫£ l·ªùi:
"""
)

def collection_exists(collection_name, client):
    try:
        client.get_collection(collection_name)
        return True
    except Exception:
        return False

def sanitize_filename(filename):
    """Chuy·ªÉn t√™n file th√†nh t√™n collection h·ª£p l·ªá"""
    name = os.path.splitext(filename)[0]
    name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
    return name[:50]

st.title("üìö QUIZ_CHAT üìö ")
st.caption("By NTChien . ƒê∆∞·ª£c s·ª≠ d·ª•ng trong tra c·ª©u v√† t·ªïng h·ª£p t√†i li·ªáu")

st.sidebar.header("C·∫•u h√¨nh API Key")
api_key = st.sidebar.text_input("Nh·∫≠p Google API Key:", type="password")

if not api_key:
    st.warning("Vui l√≤ng nh·∫≠p Google API Key ·ªü sidebar ƒë·ªÉ s·ª≠ d·ª•ng chatbot.")
    st.stop()

client = init_qdrant()
llm = init_llm(api_key)

if st.sidebar.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat"):
    st.session_state.messages = []
    st.rerun()

# Kh·ªüi t·∫°o phi√™n l√†m vi·ªác
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.collection_name = None
    st.session_state.uploaded_file = None
    st.session_state.processed = False

uploaded_file = st.file_uploader("T·∫£i l√™n file PDF", type="pdf")

if uploaded_file and (uploaded_file != st.session_state.uploaded_file):
    st.session_state.uploaded_file = uploaded_file
    st.session_state.processed = False
    st.session_state.messages = []
    st.session_state.collection_name = None


if uploaded_file and not st.session_state.processed:
    with st.status(f"ƒêang x·ª≠ l√Ω {uploaded_file.name}...", expanded=True) as status:

        st.session_state.collection_name = sanitize_filename(uploaded_file.name)
        st.write(f"T·∫°o collection: {st.session_state.collection_name}")
        

        if collection_exists(st.session_state.collection_name, client):
            st.warning(f"Collection '{st.session_state.collection_name}' ƒë√£ t·ªìn t·∫°i")
            if delete_collection(st.session_state.collection_name):
                st.write("ƒêang t·∫°o l·∫°i collection...")
        

        st.write("üìÑ Tr√≠ch xu·∫•t n·ªôi dung PDF...")
        chunks = extract_and_process_pdf(uploaded_file ,chunk_size=3000,  chunk_overlap=300,
    table_flavor='lattice')
        
        
        st.write("üß† T·∫°o embeddings...")
        embeddings = generate_embeddings(chunks, api_key)
        
        st.write("üöÄ Upload d·ªØ li·ªáu l√™n Qdrant...")
        metadata_list = [{"source": uploaded_file.name, "chunk_id": i} 
                        for i in range(len(chunks))]
        
        upload_to_qdrant(
            chunks=chunks,
            embeddings=embeddings,
            collection_name=st.session_state.collection_name,
            metadata_list=metadata_list,
            client=client,
            batch_size=100
        )
        
        status.update(label=f"‚úÖ ƒê√£ x·ª≠ l√Ω xong {uploaded_file.name}!", state="complete", expanded=False)
        st.session_state.processed = True


if not uploaded_file:
    st.info("Vui l√≤ng t·∫£i l√™n file PDF ƒë·ªÉ b·∫Øt ƒë·∫ßu")
    st.stop()


for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

def build_history(messages):
    """T·∫°o chu·ªói l·ªãch s·ª≠ h·ªôi tho·∫°i cho prompt."""
    history = ""
    for msg in messages[:-1]:
        role = "Ng∆∞·ªùi d√πng" if msg["role"] == "user" else "Tr·ª£ l√Ω"
        history += f"{role}: {msg['content']}\n"
    return history

def show_context(reranked):
    """Hi·ªÉn th·ªã ng·ªØ c·∫£nh ƒë√£ tr√≠ch xu·∫•t v·ªõi font nh·ªè."""
    with st.expander("üîé Xem ng·ªØ c·∫£nh ƒë√£ tr√≠ch xu·∫•t", expanded=False):
        for idx, (text, score) in enumerate(reranked, 1):
            st.markdown(
                f"<span style='font-size:12px'><b>ƒêo·∫°n {idx} (score={score:.3f}):</b><br>{text}</span><hr>",
                unsafe_allow_html=True
            )

def handle_user_query(prompt, api_key, client, llm, collection_name):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    history = build_history(st.session_state.messages)
    
    # B∆∞·ªõc 1: Th·ª≠ tr·∫£ l·ªùi b·∫±ng ki·∫øn th·ª©c c·ªßa LLM
    with st.spinner("üí° ƒêang suy nghƒ©..."):
        direct_prompt = prompt_direct_template.format(
            history=history,
            question=prompt
        )
        direct_response = llm.invoke(direct_prompt)
        direct_answer = direct_response.content

    # Ki·ªÉm tra n·∫øu LLM kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi
    unknow_phrases = ["kh√¥ng bi·∫øt", "kh√¥ng c√≥ th√¥ng tin", "ch∆∞a c√≥ d·ªØ li·ªáu"]
    if any(phrase in direct_answer.lower() for phrase in unknow_phrases):
        # B∆∞·ªõc 2: Ch·ªâ enhanced query khi LLM kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c
        with st.spinner("üîç ƒêang t√¨m ki·∫øm th√¥ng tin trong t√†i li·ªáu..."):
            enhanced_query = enhance_query(
                query=prompt,
                api_key=api_key,
                max_rewrites=1
            )
            enhanced_query_embedding = generate_query_embedding(enhanced_query, api_key).tolist()
            
            hits = client.search(
                collection_name=collection_name,
                query_vector=enhanced_query_embedding,
                limit=20,
                with_payload=True
            )
            candidate_texts = [hit.payload['text'] for hit in hits if hit.payload.get('text')]
            reranked = rerank_with_cross_encoder(
                query=prompt,
                candidate_texts=candidate_texts,
                k=3
            )
            context = "\n\n".join([text for text, score in reranked])
            
            # T·∫°o prompt v·ªõi ng·ªØ c·∫£nh
            formatted_prompt = prompt_template.format(
                history=history,
                context=context,
                question=prompt
            )

        with st.spinner("üí° ƒêang t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ t√†i li·ªáu..."):
            response = llm.invoke(formatted_prompt)
            answer = response.content
        
        # Hi·ªÉn th·ªã ng·ªØ c·∫£nh
        show_context(reranked)
    else:
        # S·ª≠ d·ª•ng c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp t·ª´ LLM
        answer = direct_answer

    # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        st.markdown(answer)
    st.session_state.messages.append({"role": "assistant", "content": answer})
    show_context(reranked)

if prompt := st.chat_input(f"H·ªèi v·ªÅ n·ªôi dung {uploaded_file.name}..."):
    if not st.session_state.processed:
        st.warning("Vui l√≤ng ƒë·ª£i x·ª≠ l√Ω PDF ho√†n t·∫•t")
        st.stop()
    handle_user_query(
        prompt=prompt,
        api_key=api_key,
        client=client,
        llm=llm,
        collection_name=st.session_state.collection_name
    )